"""
================================================================================
LSTM OIL PRICE FORECASTING MODEL
Backend pour application Streamlit
================================================================================
Ce module contient toute la logique d'entra√Ænement et de pr√©diction du mod√®le 
LSTM pour la pr√©vision du prix du p√©trole avec intervalles de confiance.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import yfinance as yf
import ta
from ta.momentum import RSIIndicator
from ta.volatility import AverageTrueRange, BollingerBands
from ta.trend import ADXIndicator, MACD
import warnings
warnings.filterwarnings('ignore')


# ==================== CONFIG ====================
CONFIG = {
    'OIL_TICKER': 'CL=F',
    'VIX_TICKER': '^VIX',
    'START_DATE': '2020-01-01',
    'END_DATE': '2025-01-31',
    'LOOKBACK': 60,
    'FUTURE_STEPS': 10,
    'LSTM_UNITS': 128,
    'EPOCHS': 100,
    'BATCH_SIZE': 32,
    'VALIDATION_SPLIT': 0.2,
    'PATIENCE': 20,
    'CONFIDENCE_LEVEL': 0.95,
}


# ==================== SECTION 1: T√âL√âCHARGEMENT ET PR√âPARATION DES DONN√âES ====================

def download_data(start_date, end_date, oil_ticker, vix_ticker):
    """
    T√©l√©charge les donn√©es historiques du p√©trole et du VIX depuis Yahoo Finance.
    
    Args:
        start_date (str): Date de d√©but au format 'YYYY-MM-DD'
        end_date (str): Date de fin au format 'YYYY-MM-DD'
        oil_ticker (str): Ticker du p√©trole (ex: 'CL=F')
        vix_ticker (str): Ticker du VIX (ex: '^VIX')
    
    Returns:
        pd.DataFrame: DataFrame contenant OHLCV du p√©trole + VIX Close
    """
    print("üìä T√©l√©chargement des donn√©es...")
    
    # T√©l√©charger prix du p√©trole brut
    oil_df = yf.download(oil_ticker, start=start_date, end=end_date, progress=False)
    
    # T√©l√©charger indice VIX (volatilit√© du march√©)
    vix_df = yf.download(vix_ticker, start=start_date, end=end_date, progress=False)
    
    # Merger les deux datasets sur les dates
    vix_close = vix_df[['Close']].rename(columns={'Close': 'VIX_Close'})
    df = oil_df.join(vix_close, how='left')
    
    # Remplir les NaN avec forward fill (derni√®re valeur connue)
    df = df.fillna(method='ffill').dropna()
    
    print(f"‚úÖ Donn√©es charg√©es: {len(df)} jours")
    return df


def add_technical_indicators(df):
    """
    Ajoute les indicateurs techniques principaux au DataFrame.
    Ces indicateurs capturent les patterns et la volatilit√© court terme.
    
    Indicateurs utilis√©s:
    - RSI_14: Momentum (overbought/oversold)
    - MACD: Divergence prix/tendance
    - ATR_14: Volatilit√©
    - Bollinger Bands: Support/R√©sistance
    - ADX_14: Force de la tendance
    - VROC: Momentum du volume
    
    Args:
        df (pd.DataFrame): DataFrame avec OHLCV
    
    Returns:
        pd.DataFrame: DataFrame avec indicateurs ajout√©s
    """
    print("\nüìä Calcul des indicateurs techniques (TA)...")
    
    # Extraction des s√©ries (conversion en 1D pour ta.py)
    high = df['High'].squeeze()
    low = df['Low'].squeeze()
    close = df['Close'].squeeze()
    volume = df['Volume'].squeeze()
    
    # RSI - Relative Strength Index (mesure le momentum)
    rsi = RSIIndicator(close=close, window=14)
    df['RSI_14'] = rsi.rsi()
    
    # MACD - Moving Average Convergence Divergence (tendance)
    macd = MACD(close=close, window_fast=12, window_slow=26, window_sign=9)
    df['MACD'] = macd.macd()
    df['MACD_Signal'] = macd.macd_signal()
    df['MACD_Hist'] = macd.macd_diff()
    
    # ATR - Average True Range (volatilit√©)
    atr = AverageTrueRange(high=high, low=low, close=close, window=14)
    df['ATR_14'] = atr.average_true_range()
    
    # Bollinger Bands (support/r√©sistance dynamique)
    bb = BollingerBands(close=close, window=20, window_dev=2)
    df['BB_Upper'] = bb.bollinger_hband()
    df['BB_Mid'] = bb.bollinger_mavg()
    df['BB_Lower'] = bb.bollinger_lband()
    
    # ADX - Average Directional Index (force de la tendance)
    adx = ADXIndicator(high=high, low=low, close=close, window=14)
    df['ADX_14'] = adx.adx()
    
    # Volume Price Trend (momentum du volume)
    df['VROC'] = ta.volume.volume_price_trend(close=close, volume=volume)
    
    # Return quotidien (changement % du prix)
    df['Price_Return'] = df['Close'].pct_change()
    
    print("‚úÖ Indicateurs techniques ajout√©s")
    return df


def map_market_regime(date):
    """
    Mappe les r√©gimes de march√© bas√©s sur les crises historiques.
    Cet indicateur aide le mod√®le √† comprendre le contexte √©conomique.
    
    -1 = Bear market / Crise (volatilit√© √©lev√©e, tendance baissi√®re)
     0 = Normal / Transition (p√©riode neutre)
     1 = Bull market / Recovery (tendance haussi√®re)
    
    Args:
        date (pd.Timestamp): Date √† mapper
    
    Returns:
        int: Code du r√©gime (-1, 0, ou 1)
    """
    date_val = date
    
    # COVID CRASH (2020-03 √† 2020-04)
    if pd.Timestamp('2020-03-01') <= date_val <= pd.Timestamp('2020-04-30'):
        return -1
    
    # RECOVERY POST-COVID (2020-05 √† 2021)
    elif pd.Timestamp('2020-05-01') <= date_val <= pd.Timestamp('2021-12-31'):
        return 1
    
    # INFLATION & GEOPOLITICS (2022) - Ukraine, Taux Fed
    elif pd.Timestamp('2022-01-01') <= date_val <= pd.Timestamp('2022-06-30'):
        return 0
    
    # ENERGY CRISIS & RECOVERY (2022-2023)
    elif pd.Timestamp('2022-07-01') <= date_val <= pd.Timestamp('2023-08-31'):
        return 1
    
    # BANKING CRISIS (2023-03) - SVB
    elif pd.Timestamp('2023-03-01') <= date_val <= pd.Timestamp('2023-03-31'):
        return -1
    
    # RECOVERY & STABILITY (2023-04 onwards)
    elif pd.Timestamp('2023-04-01') <= date_val <= pd.Timestamp('2025-01-31'):
        return 1
    
    # P√©riode par d√©faut
    else:
        return 0


def add_market_regime(df):
    """
    Ajoute la colonne Market_Regime au DataFrame.
    
    Args:
        df (pd.DataFrame): DataFrame
    
    Returns:
        pd.DataFrame: DataFrame avec Market_Regime ajout√©
    """
    print("\nüìç Mapping des r√©gimes de march√©...")
    df['Market_Regime'] = df.index.map(lambda x: map_market_regime(x))
    print("‚úÖ R√©gime de march√© mapp√©")
    return df


def prepare_data(df, feature_cols, target_col='Close'):
    """
    Pr√©pare et normalise les donn√©es pour l'entra√Ænement du LSTM.
    
    Args:
        df (pd.DataFrame): DataFrame nettoy√©
        feature_cols (list): Colonnes √† utiliser comme features
        target_col (str): Colonne cible (prix)
    
    Returns:
        tuple: (X_scaled, y_scaled, scaler_X, scaler_y)
    """
    print("\nüîÑ Pr√©paration et normalisation des donn√©es...")
    
    # Extraction des donn√©es
    X_data = df[feature_cols].values
    y_data = df[target_col].values.reshape(-1, 1)
    
    print(f"   Features: {len(feature_cols)}")
    print(f"   Samples: {X_data.shape[0]}")
    
    # Normalisation MinMax (ram√®ne toutes les valeurs entre 0 et 1)
    # Cela aide le LSTM √† converger plus rapidement
    scaler_X = MinMaxScaler(feature_range=(0, 1))
    X_scaled = scaler_X.fit_transform(X_data)
    
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    y_scaled = scaler_y.fit_transform(y_data)
    
    print("‚úÖ Donn√©es normalis√©es [0, 1]")
    return X_scaled, y_scaled, scaler_X, scaler_y


# ==================== SECTION 2: CR√âATION DES S√âQUENCES ====================

def create_sequences(X, y, lookback):
    """
    Cr√©e des s√©quences temporelles pour le LSTM.
    
    Le LSTM apprend en regardant des fen√™tres de 'lookback' jours pr√©c√©dents
    pour pr√©dire le prix du jour suivant.
    
    Exemple avec lookback=3:
        [jour1, jour2, jour3] ‚Üí jour4
        [jour2, jour3, jour4] ‚Üí jour5
        etc.
    
    Args:
        X (np.array): Features normalis√©es
        y (np.array): Target normalis√©
        lookback (int): Nombre de jours historiques √† regarder
    
    Returns:
        tuple: (X_seq, y_seq) - s√©quences pr√™tes pour le LSTM
    """
    X_seq, y_seq = [], []
    
    for i in range(lookback, len(X)):
        # Prendre 'lookback' jours de features
        X_seq.append(X[i-lookback:i])
        # Pr√©dire le prix du jour i
        y_seq.append(y[i, 0])
    
    return np.array(X_seq), np.array(y_seq)


def prepare_sequences(X_scaled, y_scaled, lookback, test_split):
    """
    Cr√©e les s√©quences et les divise en train/test.
    
    Args:
        X_scaled (np.array): Features normalis√©es
        y_scaled (np.array): Target normalis√©
        lookback (int): Fen√™tre temporelle
        test_split (float): % des donn√©es pour le test
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    print("\nüìä Cr√©ation des s√©quences...")
    
    # Cr√©er les s√©quences
    X_seq, y_seq = create_sequences(X_scaled, y_scaled, lookback)
    
    print(f"   S√©quences cr√©√©es: {X_seq.shape}")
    
    # Split train/test (on garde l'ordre temporel!)
    split_idx = int(len(X_seq) * (1 - test_split))
    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]
    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]
    
    print(f"   Train: {X_train.shape[0]} | Test: {X_test.shape[0]}")
    
    return X_train, X_test, y_train, y_test


# ==================== SECTION 3: CONSTRUCTION ET ENTRA√éNEMENT DU LSTM ====================

def build_lstm_model(lookback, n_features, lstm_units=128):
    """
    Construit l'architecture du mod√®le LSTM.
    
    Architecture:
    - LSTM 128 (couche 1): Capture les patterns complexes
    - Dropout 30%: R√©duit l'overfitting
    - LSTM 64 (couche 2): Affine les patterns
    - Dropout 30%
    - Dense 32 + Dense 16: Traitement suppl√©mentaire
    - Dense 1: Output (prix pr√©dit)
    
    Args:
        lookback (int): Nombre de timesteps d'entr√©e
        n_features (int): Nombre de features
        lstm_units (int): Nombre d'unit√©s LSTM dans la premi√®re couche
    
    Returns:
        tf.keras.Model: Mod√®le compil√©
    """
    print("\nüß† Construction du mod√®le LSTM...")
    
    model = Sequential([
        Input(shape=(lookback, n_features)),
        
        # Couche LSTM 1: Capture les d√©pendances long terme
        LSTM(lstm_units, activation='relu', return_sequences=True),
        Dropout(0.3),  # √âteint 30% des neurones al√©atoirement
        
        # Couche LSTM 2: Affine les patterns d√©tect√©s
        LSTM(64, activation='relu', return_sequences=False),
        Dropout(0.3),
        
        # Couches Dense: Traitement final
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1)  # Output: 1 prix pr√©dit
    ])
    
    # Compilation du mod√®le
    model.compile(
        optimizer=Adam(learning_rate=0.001),  # Optimiseur
        loss='mse',  # Loss function (Mean Squared Error)
        metrics=['mae']  # M√©trique √† surveiller
    )
    
    print(model.summary())
    return model


def train_model(model, X_train, y_train, epochs, batch_size, patience):
    """
    Entra√Æne le mod√®le LSTM.
    
    Args:
        model: Mod√®le LSTM compil√©
        X_train (np.array): Donn√©es d'entra√Ænement
        y_train (np.array): Target d'entra√Ænement
        epochs (int): Nombre d'epochs
        batch_size (int): Taille du batch
        patience (int): Patience du early stopping
    
    Returns:
        history: Historique d'entra√Ænement
    """
    print("\n‚öôÔ∏è Entra√Ænement du mod√®le...")
    
    # Early stopping: arr√™te l'entra√Ænement si val_loss ne s'am√©liore pas
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=patience,
        restore_best_weights=True
    )
    
    # Entra√Æner le mod√®le
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.15,  # 15% des donn√©es train pour validation
        callbacks=[early_stop],
        verbose=1
    )
    
    print("‚úÖ Entra√Ænement termin√©")
    return history


# ==================== SECTION 4: PR√âDICTIONS ET INTERVALLES ====================

def make_predictions(model, X_train, X_test, y_train, y_test, scaler_y):
    """
    Fait des pr√©dictions et les d√©normalise.
    
    Args:
        model: Mod√®le LSTM entra√Æn√©
        X_train, X_test: Donn√©es
        y_train, y_test: Targets
        scaler_y: Scaler pour d√©normalisation
    
    Returns:
        dict: Pr√©dictions pour train et test (actuelles et d√©normalis√©es)
    """
    print("\nüìà Pr√©dictions...")
    
    # Pr√©dictions en valeurs normalis√©es
    y_train_pred = model.predict(X_train, verbose=0)
    y_test_pred = model.predict(X_test, verbose=0)
    
    # D√©normalisation (retrouver les vrais prix)
    y_train_actual = scaler_y.inverse_transform(y_train.reshape(-1, 1))
    y_train_pred_actual = scaler_y.inverse_transform(y_train_pred)
    y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1))
    y_test_pred_actual = scaler_y.inverse_transform(y_test_pred)
    
    # Calculer les m√©triques
    train_rmse = np.sqrt(mean_squared_error(y_train_actual, y_train_pred_actual))
    test_rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_pred_actual))
    train_mae = mean_absolute_error(y_train_actual, y_train_pred_actual)
    test_mae = mean_absolute_error(y_test_actual, y_test_pred_actual)
    test_r2 = r2_score(y_test_actual, y_test_pred_actual)
    
    print(f"\nüìä M√âTRIQUES:")
    print(f"Train - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}")
    print(f"Test  - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R¬≤: {test_r2:.4f}")
    
    return {
        'y_train_actual': y_train_actual,
        'y_train_pred': y_train_pred_actual,
        'y_test_actual': y_test_actual,
        'y_test_pred': y_test_pred_actual,
        'metrics': {
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'test_r2': test_r2
        }
    }


def calculate_confidence_intervals(y_actual, y_pred, confidence_level=0.95):
    """
    Calcule les intervalles de confiance bas√©s sur l'√©cart-type des r√©sidus.
    
    Logique:
    - Calculer les erreurs (r√©sidus) du mod√®le
    - Mesurer l'√©cart-type de ces erreurs
    - Pour 95% de confiance, utiliser ¬±1.96 * std
    
    Args:
        y_actual (np.array): Valeurs r√©elles
        y_pred (np.array): Valeurs pr√©dites
        confidence_level (float): Niveau de confiance (default 0.95 = 95%)
    
    Returns:
        tuple: (lower_bound, upper_bound, std, coverage)
    """
    # Calculer les r√©sidus (erreurs)
    residuals = y_actual - y_pred
    std = np.std(residuals)
    
    # Z-score pour le niveau de confiance
    z_score = 1.96 if confidence_level == 0.95 else 1.645
    
    # Cr√©er les bornes
    lower = y_pred - z_score * std
    upper = y_pred + z_score * std
    
    # Calculer la couverture (% de vrais prix dans l'intervalle)
    coverage = np.mean((y_actual >= lower) & (y_actual <= upper))
    
    return lower, upper, std, coverage


def forecast_future(model, last_sequence, steps, n_features, scaler_y, std_val, lookback):
    """
    Pr√©dit les prix futurs jour par jour.
    
    Processus:
    1. Pr√©dire jour +1 avec les 60 derniers jours
    2. Ajouter cette pr√©diction √† la s√©quence
    3. Utiliser les 59 jours pr√©c√©dents + la pr√©diction pour jour +2
    4. R√©p√©ter
    
    Args:
        model: Mod√®le LSTM entra√Æn√©
        last_sequence (np.array): 60 derniers jours normalis√©s
        steps (int): Nombre de jours √† pr√©dire
        n_features (int): Nombre de features
        scaler_y: Scaler pour d√©normalisation
        std_val (float): √âcart-type pour les intervalles
        lookback (int): Fen√™tre temporelle
    
    Returns:
        tuple: (predictions, lower_bounds, upper_bounds)
    """
    predictions = []
    current_seq = last_sequence.copy()
    
    for _ in range(steps):
        # Pr√©dire le prix normalis√©
        pred_scaled = model.predict(
            current_seq.reshape(1, lookback, n_features),
            verbose=0
        )[0][0]
        
        # D√©normaliser
        pred_actual = scaler_y.inverse_transform([[pred_scaled]])[0][0]
        predictions.append(pred_actual)
        
        # Mettre √† jour la s√©quence pour la pr√©diction suivante
        # Supprimer le premier jour, ajouter la nouvelle pr√©diction
        current_seq = np.vstack([current_seq[1:], current_seq[-1:]])
    
    predictions = np.array(predictions)
    lower = predictions - 1.96 * std_val
    upper = predictions + 1.96 * std_val
    
    return predictions, lower, upper


# ==================== SECTION 5: PIPELINE PRINCIPAL ====================

def train_full_pipeline(config=CONFIG):
    """
    Ex√©cute le pipeline complet d'entra√Ænement.
    √Ä utiliser pour entra√Æner le mod√®le une fois.
    
    Args:
        config (dict): Configuration avec tous les param√®tres
    
    Returns:
        dict: R√©sultats complets (mod√®le, donn√©es, pr√©dictions, etc.)
    """
    # T√©l√©charger et pr√©parer les donn√©es
    df = download_data(
        config['START_DATE'],
        config['END_DATE'],
        config['OIL_TICKER'],
        config['VIX_TICKER']
    )
    
    df = add_technical_indicators(df)
    df = add_market_regime(df)
    df = df.dropna()
    
    print(f"\n‚úÖ Donn√©es nettoy√©es: {len(df)} jours")
    print(f"   Prix: ${df['Close'].values.min():.2f} - ${df['Close'].values.max():.2f}")
    
    # Pr√©parer les features
    feature_cols = ['Open', 'High', 'Low', 'Volume', 'VIX_Close', 
                    'RSI_14', 'MACD', 'ATR_14', 'ADX_14', 'VROC',
                    'BB_Upper', 'BB_Mid', 'BB_Lower', 'Market_Regime']
    
    X_scaled, y_scaled, scaler_X, scaler_y = prepare_data(df, feature_cols)
    
    # Cr√©er les s√©quences
    X_train, X_test, y_train, y_test = prepare_sequences(
        X_scaled, y_scaled,
        config['LOOKBACK'],
        config['VALIDATION_SPLIT']
    )
    
    # Construire et entra√Æner le mod√®le
    model = build_lstm_model(
        config['LOOKBACK'],
        len(feature_cols),
        config['LSTM_UNITS']
    )
    
    history = train_model(
        model, X_train, y_train,
        config['EPOCHS'],
        config['BATCH_SIZE'],
        config['PATIENCE']
    )
    
    # Faire les pr√©dictions
    preds = make_predictions(model, X_train, X_test, y_train, y_test, scaler_y)
    
    # Calculer les intervalles
    train_lower, train_upper, train_std, _ = calculate_confidence_intervals(
        preds['y_train_actual'], preds['y_train_pred'], config['CONFIDENCE_LEVEL']
    )
    test_lower, test_upper, test_std, coverage = calculate_confidence_intervals(
        preds['y_test_actual'], preds['y_test_pred'], config['CONFIDENCE_LEVEL']
    )
    
    # Pr√©dictions futures
    last_sequence = X_scaled[-config['LOOKBACK']:]
    future_pred, future_lower, future_upper = forecast_future(
        model, last_sequence, config['FUTURE_STEPS'], len(feature_cols),
        scaler_y, test_std, config['LOOKBACK']
    )
    
    print(f"\nüìä INTERVALLES DE CONFIANCE:")
    print(f"Test Std: ${test_std:.4f}")
    print(f"Coverage: {coverage*100:.2f}%")
    
    print(f"\nüîÆ PR√âDICTIONS FUTURES ({config['FUTURE_STEPS']} jours):")
    for i, (pred, low, high) in enumerate(zip(future_pred, future_lower, future_upper), 1):
        print(f"  +{i}j: ${pred:.2f} [${low:.2f}, ${high:.2f}]")
    
    # Sauvegarder le mod√®le
    model.save('lstm_oil_model.h5')
    print("\nüíæ Mod√®le sauvegard√©: 'lstm_oil_model.h5'")
    
    return {
        'model': model,
        'df': df,
        'history': history,
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'predictions': preds,
        'intervals': {
            'train': (train_lower, train_upper),
            'test': (test_lower, test_upper),
            'future': (future_lower, future_upper),
            'std': test_std,
            'coverage': coverage
        },
        'future_predictions': future_pred,
        'feature_cols': feature_cols,
        'config': config
    }


def load_and_predict(model_path='lstm_oil_model.h5'):
    """
    Charge un mod√®le sauvegard√© et fait des pr√©dictions futures.
    √Ä utiliser en production/Streamlit pour faire des pr√©dictions rapides.
    
    Args:
        model_path (str): Chemin du mod√®le sauvegard√©
    
    Returns:
        dict: Nouvelles pr√©dictions
    """
    print("üîÑ Chargement du mod√®le...")
    model = load_model(model_path)
    
    # T√©l√©charger les derni√®res donn√©es
    df = download_data(
        CONFIG['START_DATE'],
        CONFIG['END_DATE'],
        CONFIG['OIL_TICKER'],
        CONFIG['VIX_TICKER']
    )
    
    df = add_technical_indicators(df)
    df = add_market_regime(df)
    df = df.dropna()
    
    print("‚úÖ Mod√®le charg√© et donn√©es r√©centes r√©cup√©r√©es")
    
    return {'model': model, 'df': df}


if __name__ == "__main__":
    """
    Point d'entr√©e principal - √† ex√©cuter une seule fois pour entra√Æner le mod√®le.
    """
    results = train_full_pipeline(CONFIG)
    print("\n" + "="*60)
    print("‚úÖ PIPELINE COMPLET EX√âCUT√â")
    print("="*60)