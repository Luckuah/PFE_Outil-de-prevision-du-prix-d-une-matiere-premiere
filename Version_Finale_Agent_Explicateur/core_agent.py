import json
from datetime import datetime
from data_provider import aggregate_market_data
from llm_client import call_ollama

# ============================================
# AGENT D'ANALYSE PRINCIPAL
# ============================================

def analyze_market(
    user_query: str,
    documents: list[dict],
    lstm_prediction: float,
    lstm_prediction_date: str
) -> dict:
    """
    Agent principal qui analyse le march√© et g√©n√®re une pr√©diction √† 10 jours.
    """
    print("ü§ñ D√©marrage de l'analyse...")
    
    # 1. R√©cup√©rer les donn√©es de march√©
    market_data = aggregate_market_data()
    
    # 2. Formater les documents
    docs_text = "\n".join([
        f"Document {i+1}: {doc.get('title', 'Sans titre')}\n{doc.get('content', doc.get('summary', 'Pas de contenu'))}\n"
        for i, doc in enumerate(documents[:5])
    ])
    
    # 3. Construire le prompt
    system_prompt = """Tu es un analyste financier expert sp√©cialis√© dans les march√©s p√©troliers (Crude Oil Futures - CL=F).
Ta mission est d'analyser les donn√©es de march√© multi-timeframe, les actualit√©s pertinentes, et une pr√©diction LSTM 
pour fournir une pr√©diction de prix √† 10 jours avec une explication d√©taill√©e.

R√©ponds TOUJOURS en JSON avec ce format exact:
{
    "predicted_price_10d": <float>,
    "confidence": "HIGH" | "MEDIUM" | "LOW",
    "explanation": "<explication d√©taill√©e en fran√ßais>",
    "key_factors": ["facteur1", "facteur2", "facteur3"]
}"""

    analysis_prompt = f"""
## Question de l'utilisateur:
{user_query}

## Donn√©es de march√© actuelles (CL=F - Crude Oil Futures):
{json.dumps(market_data, indent=2, default=str)}

## Pr√©diction du mod√®le LSTM:
- Prix pr√©dit: ${lstm_prediction:.2f}
- Date de pr√©diction: {lstm_prediction_date}

## Documents/Articles pertinents fournis par l'agent de recherche:
{docs_text}

## Ta mission:
1. Analyse les tendances sur les 3 timeframes (15min, 4h, daily)
2. Prends en compte la pr√©diction LSTM comme un indicateur parmi d'autres
3. Int√®gre les informations des documents pour contextualiser
4. R√©ponds √† la question de l'utilisateur
5. Fournis ta pr√©diction de prix √† 10 jours avec explication

R√©ponds en JSON valide.
"""
    
    # 4. Appeler le LLM
    print("üí≠ Appel au LLM Qwen...")
    raw_response = call_ollama(analysis_prompt, system_prompt)
    
    # 5. Parser la r√©ponse
    try:
        # Essayer d'extraire le JSON de la r√©ponse
        json_start = raw_response.find('{')
        json_end = raw_response.rfind('}') + 1
        if json_start != -1 and json_end > json_start:
            json_str = raw_response[json_start:json_end]
            parsed_response = json.loads(json_str)
        else:
            parsed_response = {
                "predicted_price_10d": lstm_prediction,  # Fallback sur LSTM
                "confidence": "LOW",
                "explanation": raw_response,
                "key_factors": ["Parsing error - raw response returned"]
            }
    except json.JSONDecodeError:
        parsed_response = {
            "predicted_price_10d": lstm_prediction,
            "confidence": "LOW", 
            "explanation": raw_response,
            "key_factors": ["JSON parsing failed"]
        }
    
    # 6. Ajouter m√©tadonn√©es
    parsed_response["timestamp"] = datetime.now().isoformat()
    parsed_response["market_data_summary"] = market_data.get("timeframes", {})
    parsed_response["lstm_input"] = lstm_prediction
    
    return parsed_response