"""
RAG Agent for semantic search and answer generation
"""
import numpy as np
import pandas as pd
import faiss
import pickle
from pathlib import Path
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

from utils.logger import get_logger
from utils.config_loader import get_config

logger = get_logger(__name__)


class RAGAgent:
    """RAG Agent: FAISS Semantic Search + Qwen Generation"""
    
    def __init__(self):
        """Initialize RAG Agent"""
        self.config = get_config()
        
        # 1. Embedding Model (Sentence-BERT)
        emb_model_name = self.config.get(
            'models.embeddings.name', 
            'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
        )
        logger.info(f"ðŸ”¢ Loading embedding model: {emb_model_name}")
        self.embedding_model = SentenceTransformer(emb_model_name)
        self.dimension = self.config.get('models.embeddings.dimension', 768)
        
        # 2. LLM for Generation (Qwen via llama-cpp)
        repo_id = self.config.get('models.rag.repo_id', "Qwen/Qwen2.5-0.5B-Instruct-GGUF")
        filename = self.config.get('models.rag.filename', "*q8_0.gguf")
        n_ctx = self.config.get('models.rag.n_ctx', 4096)
        
        logger.info(f"ðŸ¤– Loading LLM for RAG: {repo_id}")
        self.llm = Llama.from_pretrained(
            repo_id=repo_id,
            filename=filename,
            n_gpu_layers=-1,  # Use GPU if available
            n_ctx=n_ctx,
            verbose=False
        )
        logger.info("âœ… LLM loaded successfully")
        
        # FAISS setup
        self.index = None
        self.documents_df = None
        
        # Paths
        self.index_path = Path(self.config.get('paths.faiss_index', './data/faiss_index'))
        self.index_path.mkdir(parents=True, exist_ok=True)
        
        self.index_file = self.index_path / 'faiss_index.bin'
        self.docs_file = self.index_path / 'documents.pkl'
        
        logger.info("âœ… RAG Agent initialized")
    
    def build_index(self, df: pd.DataFrame, 
                   text_column: str = 'article_content',
                   batch_size: int = 32):
        """
        Build FAISS index from DataFrame
        
        Args:
            df: DataFrame with articles
            text_column: Column containing text to embed
            batch_size: Batch size for embedding
        """
        logger.info(f"ðŸ“š Building FAISS index from {len(df)} documents...")
        
        if df.empty:
            logger.warning("âš ï¸ Empty DataFrame provided")
            return
        
        # VÃ©rifier que la colonne existe
        if text_column not in df.columns:
            logger.error(f"âŒ Column '{text_column}' not found in DataFrame")
            logger.error(f"   Available columns: {df.columns.tolist()}")
            return
        
        # PrÃ©parer les textes
        texts = df[text_column].fillna('').tolist()
        
        # GÃ©nÃ©rer les embeddings par batch
        logger.info("ðŸ”¢ Generating embeddings...")
        embeddings = self.embedding_model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        # CrÃ©er l'index FAISS
        logger.info("ðŸ” Creating FAISS index...")
        embeddings = embeddings.astype('float32')
        
        # Utiliser IndexFlatIP pour Inner Product (cosine similarity)
        self.index = faiss.IndexFlatIP(self.dimension)
        self.index.add(embeddings)
        
        # Sauvegarder les documents
        self.documents_df = df.copy()
        
        # Sauvegarder sur disque
        self.save_index()
        
        logger.info(f"âœ… FAISS index built with {self.index.ntotal} vectors")
        logger.info(f"   Dimension: {self.dimension}")
        logger.info(f"   Saved to: {self.index_file}")
    
    def save_index(self):
        """Save FAISS index and documents to disk"""
        try:
            # Sauvegarder l'index FAISS
            faiss.write_index(self.index, str(self.index_file))
            
            # Sauvegarder le DataFrame
            with open(self.docs_file, 'wb') as f:
                pickle.dump(self.documents_df, f)
            
            logger.info(f"ðŸ’¾ Index saved to {self.index_file}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save index: {e}")
    
    def load_index(self) -> bool:
        """
        Load FAISS index from disk
        
        Returns:
            True if successful, False otherwise
        """
        try:
            if not self.index_file.exists() or not self.docs_file.exists():
                logger.warning("âš ï¸ Index files not found")
                return False
            
            logger.info(f"ðŸ“‚ Loading FAISS index from {self.index_file}")
            
            # Charger l'index FAISS
            self.index = faiss.read_index(str(self.index_file))
            
            # Charger le DataFrame
            with open(self.docs_file, 'rb') as f:
                self.documents_df = pickle.load(f)
            
            logger.info(f"âœ… Loaded index with {self.index.ntotal} vectors")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load index: {e}")
            return False
    
    def search(self, query: str, k: int = 5) -> pd.DataFrame:
        """
        Semantic search using FAISS
        
        Args:
            query: Search query
            k: Number of results to return
            
        Returns:
            DataFrame with top-k documents and similarity scores
        """
        if self.index is None:
            logger.warning("âš ï¸ Index not loaded, attempting to load...")
            if not self.load_index():
                logger.error("âŒ No index available. Build index first.")
                return pd.DataFrame()
        
        # GÃ©nÃ©rer embedding de la requÃªte
        query_embedding = self.embedding_model.encode(
            [query], 
            normalize_embeddings=True
        )
        
        # Rechercher dans FAISS
        distances, indices = self.index.search(
            query_embedding.astype('float32'), 
            k
        )
        
        # RÃ©cupÃ©rer les documents
        if self.documents_df is None or len(indices[0]) == 0:
            return pd.DataFrame()
        
        top_docs = self.documents_df.iloc[indices[0]].copy()
        top_docs['similarity_score'] = distances[0]
        
        return top_docs
    
    def answer_with_context(self, query: str, k: int = 5) -> str:
        """
        RAG: Retrieve documents + Generate answer
        
        Args:
            query: Question to answer
            k: Number of documents to retrieve
            
        Returns:
            Generated answer
        """
        logger.info(f"ðŸ” RAG Query: {query}")
        
        # 1. Retrieval - Recherche sÃ©mantique
        docs = self.search(query, k=k)
        
        if docs.empty:
            return "I couldn't find relevant documents to answer this question."
        
        # 2. Construction du contexte
        context_text = "\n\n".join([
            f"Document {i+1} (Title: {row.get('article_title', 'N/A')}): "
            f"{row.get('article_content', '')[:600]}"
            for i, (_, row) in enumerate(docs.iterrows())
        ])
        
        # 3. Generation - RÃ©ponse par le LLM
        prompt = f"""
        You are an expert oil market and geopolitics analyst.

        RULES:
        - Use ONLY the information from the CONTEXT below.
        - Do NOT rely on prior knowledge.
        - If the answer is not explicitly supported by the context, say:
        "I don't know based on the provided documents."

        TASK:
        Answer the question focusing on oil supply, demand, geopolitics, sanctions,
        production, transport, OPEC decisions, or market expectations. 

        CONTEXT:
        {context_text}

        QUESTION:
        {query}

        ANSWER (cite document ids, urls  when relevant from your {context_text} ):"""
        
        logger.info("ðŸ¤– Generating answer with LLM...")
        
        try:
            response = self.llm.create_chat_completion(
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert analyst on oil markets and geopolique news."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0.2,
                max_tokens=500
            )
            
            answer = response['choices'][0]['message']['content']
            logger.info("âœ… Answer generated")
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate answer: {e}")
            return f"Error generating answer: {str(e)}"
    
    def get_index_stats(self) -> dict:
        """
        Get statistics about the current index
        
        Returns:
            Dictionary with index statistics
        """
        if self.index is None:
            return {
                'status': 'No index loaded',
                'num_vectors': 0,
                'dimension': self.dimension
            }
        
        return {
            'status': 'Index loaded',
            'num_vectors': self.index.ntotal,
            'dimension': self.dimension,
            'num_documents': len(self.documents_df) if self.documents_df is not None else 0,
            'index_file': str(self.index_file),
            'docs_file': str(self.docs_file)
        }