"""
================================================================================
LSTM OIL PRICE FORECASTING MODEL
Backend pour application Streamlit
================================================================================
Ce module contient toute la logique d'entraÃ®nement et de prÃ©diction du modÃ¨le 
LSTM pour la prÃ©vision du prix du pÃ©trole avec intervalles de confiance.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import yfinance as yf
import ta
from ta.momentum import RSIIndicator
from ta.volatility import AverageTrueRange, BollingerBands
from ta.trend import ADXIndicator, MACD
import warnings
warnings.filterwarnings('ignore')


# ==================== CONFIG ====================
CONFIG = {
    'OIL_TICKER': 'CL=F',
    'VIX_TICKER': '^VIX',
    'START_DATE': '2020-01-01',
    'END_DATE': '2025-01-31',
    'LOOKBACK': 60,
    'FUTURE_STEPS': 10,
    'LSTM_UNITS': 128,
    'EPOCHS': 100,
    'BATCH_SIZE': 32,
    'VALIDATION_SPLIT': 0.2,
    'PATIENCE': 20,
    'CONFIDENCE_LEVEL': 0.95,
}


# ==================== SECTION 1: TÃ‰LÃ‰CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES ====================

def download_data(start_date, end_date, oil_ticker, vix_ticker):
    """
    TÃ©lÃ©charge les donnÃ©es historiques du pÃ©trole et du VIX depuis Yahoo Finance.
    
    Args:
        start_date (str): Date de dÃ©but au format 'YYYY-MM-DD'
        end_date (str): Date de fin au format 'YYYY-MM-DD'
        oil_ticker (str): Ticker du pÃ©trole (ex: 'CL=F')
        vix_ticker (str): Ticker du VIX (ex: '^VIX')
    
    Returns:
        pd.DataFrame: DataFrame contenant OHLCV du pÃ©trole + VIX Close
    """
    print("ðŸ“Š TÃ©lÃ©chargement des donnÃ©es...")
    
    # TÃ©lÃ©charger prix du pÃ©trole brut
    oil_df = yf.download(oil_ticker, start=start_date, end=end_date, progress=False)
    
    # TÃ©lÃ©charger indice VIX (volatilitÃ© du marchÃ©)
    vix_df = yf.download(vix_ticker, start=start_date, end=end_date, progress=False)
    
    # Merger les deux datasets sur les dates
    vix_close = vix_df[['Close']].rename(columns={'Close': 'VIX_Close'})
    df = oil_df.join(vix_close, how='left')
    
    # Remplir les NaN avec forward fill (derniÃ¨re valeur connue)
    df = df.fillna(method='ffill').dropna()
    
    print(f"âœ… DonnÃ©es chargÃ©es: {len(df)} jours")
    return df


def add_technical_indicators(df):
    """
    Ajoute les indicateurs techniques principaux au DataFrame.
    Ces indicateurs capturent les patterns et la volatilitÃ© court terme.
    
    Indicateurs utilisÃ©s:
    - RSI_14: Momentum (overbought/oversold)
    - MACD: Divergence prix/tendance
    - ATR_14: VolatilitÃ©
    - Bollinger Bands: Support/RÃ©sistance
    - ADX_14: Force de la tendance
    - VROC: Momentum du volume
    
    Args:
        df (pd.DataFrame): DataFrame avec OHLCV
    
    Returns:
        pd.DataFrame: DataFrame avec indicateurs ajoutÃ©s
    """
    print("\nðŸ“Š Calcul des indicateurs techniques (TA)...")
    
    # Extraction des sÃ©ries (conversion en 1D pour ta.py)
    high = df['High'].squeeze()
    low = df['Low'].squeeze()
    close = df['Close'].squeeze()
    volume = df['Volume'].squeeze()
    
    # RSI - Relative Strength Index (mesure le momentum)
    rsi = RSIIndicator(close=close, window=14)
    df['RSI_14'] = rsi.rsi()
    
    # MACD - Moving Average Convergence Divergence (tendance)
    macd = MACD(close=close, window_fast=12, window_slow=26, window_sign=9)
    df['MACD'] = macd.macd()
    df['MACD_Signal'] = macd.macd_signal()
    df['MACD_Hist'] = macd.macd_diff()
    
    # ATR - Average True Range (volatilitÃ©)
    atr = AverageTrueRange(high=high, low=low, close=close, window=14)
    df['ATR_14'] = atr.average_true_range()
    
    # Bollinger Bands (support/rÃ©sistance dynamique)
    bb = BollingerBands(close=close, window=20, window_dev=2)
    df['BB_Upper'] = bb.bollinger_hband()
    df['BB_Mid'] = bb.bollinger_mavg()
    df['BB_Lower'] = bb.bollinger_lband()
    
    # ADX - Average Directional Index (force de la tendance)
    adx = ADXIndicator(high=high, low=low, close=close, window=14)
    df['ADX_14'] = adx.adx()
    
    # Volume Price Trend (momentum du volume)
    df['VROC'] = ta.volume.volume_price_trend(close=close, volume=volume)
    
    # Return quotidien (changement % du prix)
    df['Price_Return'] = df['Close'].pct_change()
    
    print("âœ… Indicateurs techniques ajoutÃ©s")
    return df





def add_market_regime(df, crises_dict=None):
    """
    Ajoute la colonne Market_Regime basÃ©e sur les crises fournies.
    Si crises_dict est None, utilise une logique par dÃ©faut ou 0.
    """
    print("\nðŸ“ Mapping des rÃ©gimes de marchÃ© dynamiques...")
    
    # On initialise tout Ã  1 (Bull/Normal) par dÃ©faut
    df['Market_Regime'] = 1 
    
    if crises_dict:
        for nom, (debut, fin) in crises_dict.items():
            # On passe en rÃ©gime de crise (-1) pour toutes les plages sÃ©lectionnÃ©es
            mask = (df.index >= pd.to_datetime(debut)) & (df.index <= pd.to_datetime(fin))
            df.loc[mask, 'Market_Regime'] = -1
            
    print(f"âœ… RÃ©gime de marchÃ© mappÃ© avec {len(crises_dict) if crises_dict else 0} zones de crise.")
    return df


def prepare_data(df, feature_cols, target_col='Close'):
    """
    PrÃ©pare et normalise les donnÃ©es pour l'entraÃ®nement du LSTM.
    
    Args:
        df (pd.DataFrame): DataFrame nettoyÃ©
        feature_cols (list): Colonnes Ã  utiliser comme features
        target_col (str): Colonne cible (prix)
    
    Returns:
        tuple: (X_scaled, y_scaled, scaler_X, scaler_y)
    """
    print("\nðŸ”„ PrÃ©paration et normalisation des donnÃ©es...")
    
    # Extraction des donnÃ©es
    X_data = df[feature_cols].values
    y_data = df[target_col].values.reshape(-1, 1)
    
    print(f"   Features: {len(feature_cols)}")
    print(f"   Samples: {X_data.shape[0]}")
    
    # Normalisation MinMax (ramÃ¨ne toutes les valeurs entre 0 et 1)
    # Cela aide le LSTM Ã  converger plus rapidement
    scaler_X = MinMaxScaler(feature_range=(0, 1))
    X_scaled = scaler_X.fit_transform(X_data)
    
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    y_scaled = scaler_y.fit_transform(y_data)
    
    print("âœ… DonnÃ©es normalisÃ©es [0, 1]")
    return X_scaled, y_scaled, scaler_X, scaler_y


# ==================== SECTION 2: CRÃ‰ATION DES SÃ‰QUENCES ====================

def create_sequences(X, y, lookback):
    """
    CrÃ©e des sÃ©quences temporelles pour le LSTM.
    
    Le LSTM apprend en regardant des fenÃªtres de 'lookback' jours prÃ©cÃ©dents
    pour prÃ©dire le prix du jour suivant.
    
    Exemple avec lookback=3:
        [jour1, jour2, jour3] â†’ jour4
        [jour2, jour3, jour4] â†’ jour5
        etc.
    
    Args:
        X (np.array): Features normalisÃ©es
        y (np.array): Target normalisÃ©
        lookback (int): Nombre de jours historiques Ã  regarder
    
    Returns:
        tuple: (X_seq, y_seq) - sÃ©quences prÃªtes pour le LSTM
    """
    X_seq, y_seq = [], []
    
    for i in range(lookback, len(X)):
        # Prendre 'lookback' jours de features
        X_seq.append(X[i-lookback:i])
        # PrÃ©dire le prix du jour i
        y_seq.append(y[i, 0])
    
    return np.array(X_seq), np.array(y_seq)


def prepare_sequences(X_scaled, y_scaled, lookback, test_split):
    """
    CrÃ©e les sÃ©quences et les divise en train/test.
    
    Args:
        X_scaled (np.array): Features normalisÃ©es
        y_scaled (np.array): Target normalisÃ©
        lookback (int): FenÃªtre temporelle
        test_split (float): % des donnÃ©es pour le test
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    print("\nðŸ“Š CrÃ©ation des sÃ©quences...")
    
    # CrÃ©er les sÃ©quences
    X_seq, y_seq = create_sequences(X_scaled, y_scaled, lookback)
    
    print(f"   SÃ©quences crÃ©Ã©es: {X_seq.shape}")
    
    # Split train/test (on garde l'ordre temporel!)
    split_idx = int(len(X_seq) * (1 - test_split))
    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]
    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]
    
    print(f"   Train: {X_train.shape[0]} | Test: {X_test.shape[0]}")
    
    return X_train, X_test, y_train, y_test


# ==================== SECTION 3: CONSTRUCTION ET ENTRAÃŽNEMENT DU LSTM ====================

def build_lstm_model(lookback, n_features, lstm_units=128):
    """
    Construit l'architecture du modÃ¨le LSTM.
    
    Architecture:
    - LSTM 128 (couche 1): Capture les patterns complexes
    - Dropout 30%: RÃ©duit l'overfitting
    - LSTM 64 (couche 2): Affine les patterns
    - Dropout 30%
    - Dense 32 + Dense 16: Traitement supplÃ©mentaire
    - Dense 1: Output (prix prÃ©dit)
    
    Args:
        lookback (int): Nombre de timesteps d'entrÃ©e
        n_features (int): Nombre de features
        lstm_units (int): Nombre d'unitÃ©s LSTM dans la premiÃ¨re couche
    
    Returns:
        tf.keras.Model: ModÃ¨le compilÃ©
    """
    print("\nðŸ§  Construction du modÃ¨le LSTM...")
    
    model = Sequential([
        Input(shape=(lookback, n_features)),
        
        # Couche LSTM 1: Capture les dÃ©pendances long terme
        LSTM(lstm_units, activation='relu', return_sequences=True),
        Dropout(0.3),  # Ã‰teint 30% des neurones alÃ©atoirement
        
        # Couche LSTM 2: Affine les patterns dÃ©tectÃ©s
        LSTM(64, activation='relu', return_sequences=False),
        Dropout(0.3),
        
        # Couches Dense: Traitement final
        Dense(32, activation='relu'),
        Dense(16, activation='relu'),
        Dense(1)  # Output: 1 prix prÃ©dit
    ])
    
    # Compilation du modÃ¨le
    model.compile(
        optimizer=Adam(learning_rate=0.001),  # Optimiseur
        loss='mse',  # Loss function (Mean Squared Error)
        metrics=['mae']  # MÃ©trique Ã  surveiller
    )
    
    print(model.summary())
    return model


def train_model(model, X_train, y_train, epochs, batch_size, patience):
    """
    EntraÃ®ne le modÃ¨le LSTM.
    
    Args:
        model: ModÃ¨le LSTM compilÃ©
        X_train (np.array): DonnÃ©es d'entraÃ®nement
        y_train (np.array): Target d'entraÃ®nement
        epochs (int): Nombre d'epochs
        batch_size (int): Taille du batch
        patience (int): Patience du early stopping
    
    Returns:
        history: Historique d'entraÃ®nement
    """
    print("\nâš™ï¸ EntraÃ®nement du modÃ¨le...")
    
    # Early stopping: arrÃªte l'entraÃ®nement si val_loss ne s'amÃ©liore pas
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=patience,
        restore_best_weights=True
    )
    
    # EntraÃ®ner le modÃ¨le
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.15,  # 15% des donnÃ©es train pour validation
        callbacks=[early_stop],
        verbose=1
    )
    
    print("âœ… EntraÃ®nement terminÃ©")
    return history


# ==================== SECTION 4: PRÃ‰DICTIONS ET INTERVALLES ====================

def make_predictions(model, X_train, X_test, y_train, y_test, scaler_y):
    """
    Fait des prÃ©dictions et les dÃ©normalise.
    
    Args:
        model: ModÃ¨le LSTM entraÃ®nÃ©
        X_train, X_test: DonnÃ©es
        y_train, y_test: Targets
        scaler_y: Scaler pour dÃ©normalisation
    
    Returns:
        dict: PrÃ©dictions pour train et test (actuelles et dÃ©normalisÃ©es)
    """
    print("\nðŸ“ˆ PrÃ©dictions...")
    
    # PrÃ©dictions en valeurs normalisÃ©es
    y_train_pred = model.predict(X_train, verbose=0)
    y_test_pred = model.predict(X_test, verbose=0)
    
    # DÃ©normalisation (retrouver les vrais prix)
    y_train_actual = scaler_y.inverse_transform(y_train.reshape(-1, 1))
    y_train_pred_actual = scaler_y.inverse_transform(y_train_pred)
    y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1))
    y_test_pred_actual = scaler_y.inverse_transform(y_test_pred)
    
    # Calculer les mÃ©triques
    train_rmse = np.sqrt(mean_squared_error(y_train_actual, y_train_pred_actual))
    test_rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_pred_actual))
    train_mae = mean_absolute_error(y_train_actual, y_train_pred_actual)
    test_mae = mean_absolute_error(y_test_actual, y_test_pred_actual)
    test_r2 = r2_score(y_test_actual, y_test_pred_actual)
    
    print(f"\nðŸ“Š MÃ‰TRIQUES:")
    print(f"Train - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}")
    print(f"Test  - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, RÂ²: {test_r2:.4f}")
    
    return {
        'y_train_actual': y_train_actual,
        'y_train_pred': y_train_pred_actual,
        'y_test_actual': y_test_actual,
        'y_test_pred': y_test_pred_actual,
        'metrics': {
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'test_r2': test_r2
        }
    }


def calculate_confidence_intervals(y_actual, y_pred, confidence_level=0.95):
    """
    Calcule les intervalles de confiance basÃ©s sur l'Ã©cart-type des rÃ©sidus.
    
    Logique:
    - Calculer les erreurs (rÃ©sidus) du modÃ¨le
    - Mesurer l'Ã©cart-type de ces erreurs
    - Pour 95% de confiance, utiliser Â±1.96 * std
    
    Args:
        y_actual (np.array): Valeurs rÃ©elles
        y_pred (np.array): Valeurs prÃ©dites
        confidence_level (float): Niveau de confiance (default 0.95 = 95%)
    
    Returns:
        tuple: (lower_bound, upper_bound, std, coverage)
    """
    # Calculer les rÃ©sidus (erreurs)
    residuals = y_actual - y_pred
    std = np.std(residuals)
    
    # Z-score pour le niveau de confiance
    z_score = 1.96 if confidence_level == 0.95 else 1.645
    
    # CrÃ©er les bornes
    lower = y_pred - z_score * std
    upper = y_pred + z_score * std
    
    # Calculer la couverture (% de vrais prix dans l'intervalle)
    coverage = np.mean((y_actual >= lower) & (y_actual <= upper))
    
    return lower, upper, std, coverage


def forecast_future(model, last_sequence, steps, n_features, scaler_y, std_val, lookback):
    """
    PrÃ©dit les prix futurs jour par jour.
    
    Processus:
    1. PrÃ©dire jour +1 avec les 60 derniers jours
    2. Ajouter cette prÃ©diction Ã  la sÃ©quence
    3. Utiliser les 59 jours prÃ©cÃ©dents + la prÃ©diction pour jour +2
    4. RÃ©pÃ©ter
    
    Args:
        model: ModÃ¨le LSTM entraÃ®nÃ©
        last_sequence (np.array): 60 derniers jours normalisÃ©s
        steps (int): Nombre de jours Ã  prÃ©dire
        n_features (int): Nombre de features
        scaler_y: Scaler pour dÃ©normalisation
        std_val (float): Ã‰cart-type pour les intervalles
        lookback (int): FenÃªtre temporelle
    
    Returns:
        tuple: (predictions, lower_bounds, upper_bounds)
    """
    predictions = []
    current_seq = last_sequence.copy()
    
    for _ in range(steps):
        # PrÃ©dire le prix normalisÃ©
        pred_scaled = model.predict(
            current_seq.reshape(1, lookback, n_features),
            verbose=0
        )[0][0]
        
        # DÃ©normaliser
        pred_actual = scaler_y.inverse_transform([[pred_scaled]])[0][0]
        predictions.append(pred_actual)
        
        # Mettre Ã  jour la sÃ©quence pour la prÃ©diction suivante
        # Supprimer le premier jour, ajouter la nouvelle prÃ©diction
        current_seq = np.vstack([current_seq[1:], current_seq[-1:]])
    
    predictions = np.array(predictions)
    lower = predictions - 1.96 * std_val
    upper = predictions + 1.96 * std_val
    
    return predictions, lower, upper


# ==================== SECTION 5: PIPELINE PRINCIPAL ====================

def train_full_pipeline(config=CONFIG,crises_dict=None):
    """
    ExÃ©cute le pipeline complet d'entraÃ®nement.
    Ã€ utiliser pour entraÃ®ner le modÃ¨le une fois.
    
    Args:
        config (dict): Configuration avec tous les paramÃ¨tres
    
    Returns:
        dict: RÃ©sultats complets (modÃ¨le, donnÃ©es, prÃ©dictions, etc.)
    """
    # TÃ©lÃ©charger et prÃ©parer les donnÃ©es
    df = download_data(
        config['START_DATE'],
        config['END_DATE'],
        config['OIL_TICKER'],
        config['VIX_TICKER']
    )
    
    df = add_technical_indicators(df)
    df = add_market_regime(df,crises_dict)
    df = df.dropna()
    
    print(f"\nâœ… DonnÃ©es nettoyÃ©es: {len(df)} jours")
    print(f"   Prix: ${df['Close'].values.min():.2f} - ${df['Close'].values.max():.2f}")
    
    # PrÃ©parer les features
    feature_cols = ['Open', 'High', 'Low', 'Volume', 'VIX_Close', 
                    'RSI_14', 'MACD', 'ATR_14', 'ADX_14', 'VROC',
                    'BB_Upper', 'BB_Mid', 'BB_Lower', 'Market_Regime']
    
    X_scaled, y_scaled, scaler_X, scaler_y = prepare_data(df, feature_cols)
    
    # CrÃ©er les sÃ©quences
    X_train, X_test, y_train, y_test = prepare_sequences(
        X_scaled, y_scaled,
        config['LOOKBACK'],
        config['VALIDATION_SPLIT']
    )
    
    # Construire et entraÃ®ner le modÃ¨le
    model = build_lstm_model(
        config['LOOKBACK'],
        len(feature_cols),
        config['LSTM_UNITS']
    )
    
    history = train_model(
        model, X_train, y_train,
        config['EPOCHS'],
        config['BATCH_SIZE'],
        config['PATIENCE']
    )
    
    # Faire les prÃ©dictions
    preds = make_predictions(model, X_train, X_test, y_train, y_test, scaler_y)
    
    # Calculer les intervalles
    train_lower, train_upper, train_std, _ = calculate_confidence_intervals(
        preds['y_train_actual'], preds['y_train_pred'], config['CONFIDENCE_LEVEL']
    )
    test_lower, test_upper, test_std, coverage = calculate_confidence_intervals(
        preds['y_test_actual'], preds['y_test_pred'], config['CONFIDENCE_LEVEL']
    )
    
    # PrÃ©dictions futures
    last_sequence = X_scaled[-config['LOOKBACK']:]
    future_pred, future_lower, future_upper = forecast_future(
        model, last_sequence, config['FUTURE_STEPS'], len(feature_cols),
        scaler_y, test_std, config['LOOKBACK']
    )
    
    print(f"\nðŸ“Š INTERVALLES DE CONFIANCE:")
    print(f"Test Std: ${test_std:.4f}")
    print(f"Coverage: {coverage*100:.2f}%")
    
    print(f"\nðŸ”® PRÃ‰DICTIONS FUTURES ({config['FUTURE_STEPS']} jours):")
    for i, (pred, low, high) in enumerate(zip(future_pred, future_lower, future_upper), 1):
        print(f"  +{i}j: ${pred:.2f} [${low:.2f}, ${high:.2f}]")
    
    # Sauvegarder le modÃ¨le
    model.save('lstm_oil_model.h5')
    print("\nðŸ’¾ ModÃ¨le sauvegardÃ©: 'lstm_oil_model.h5'")
    
    return {
        'model': model,
        'df': df,
        'history': history,
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'predictions': preds,
        'intervals': {
            'train': (train_lower, train_upper),
            'test': (test_lower, test_upper),
            'future': (future_lower, future_upper),
            'std': test_std,
            'coverage': coverage
        },
        'future_predictions': future_pred,
        'feature_cols': feature_cols,
        'config': config
    }


def load_and_predict(model_path='lstm_oil_model.h5'):
    """
    Charge un modÃ¨le sauvegardÃ© et fait des prÃ©dictions futures.
    Ã€ utiliser en production/Streamlit pour faire des prÃ©dictions rapides.
    
    Args:
        model_path (str): Chemin du modÃ¨le sauvegardÃ©
    
    Returns:
        dict: Nouvelles prÃ©dictions
    """
    print("ðŸ”„ Chargement du modÃ¨le...")
    model = load_model(model_path, compile=False)
    
    # TÃ©lÃ©charger les derniÃ¨res donnÃ©es
    df = download_data(
        CONFIG['START_DATE'],
        CONFIG['END_DATE'],
        CONFIG['OIL_TICKER'],
        CONFIG['VIX_TICKER']
    )
    
    df = add_technical_indicators(df)
    df = add_market_regime(df)
    df = df.dropna()
    
    print("âœ… ModÃ¨le chargÃ© et donnÃ©es rÃ©centes rÃ©cupÃ©rÃ©es")
    
    return {'model': model, 'df': df}


if __name__ == "__main__":
    """
    Point d'entrÃ©e principal - Ã  exÃ©cuter une seule fois pour entraÃ®ner le modÃ¨le.
    """
    results = train_full_pipeline(CONFIG)
    print("\n" + "="*60)
    print("âœ… PIPELINE COMPLET EXÃ‰CUTÃ‰")
    print("="*60)